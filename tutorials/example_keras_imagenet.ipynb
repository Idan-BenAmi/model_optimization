{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {},
   "source": [
    "# Quantization using the Model Compression Toolkit - ImageNet dataset example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762",
   "metadata": {},
   "source": [
    "This quick start guide covers how to use the Model Compression Toolkit (MCT) for quantizing a pre-trained model on ImageNet. We will do so by giving an example, quantizing a pre-trained model, then evaluating the accuracy on ImageNet dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e9543-d356-412f-acf1-c2ecad553e06",
   "metadata": {},
   "source": [
    "In this tutorial we will cover:\n",
    "\n",
    "1. Loading and preprocessing ImageNet validation dataset\n",
    "2. Loading and preprocessing ImageNet unlabeled representative dataset\n",
    "3. Post-Training-Quantization using MCT\n",
    "4. Accuracy evaluation of the floating-point and the quantized models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d",
   "metadata": {},
   "source": [
    "Install the relevant packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q model-compression-toolkit\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import model_compression_toolkit as mct\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a8ca4-6c62-4624-a1ec-662c03dde902",
   "metadata": {},
   "source": [
    "Define dataset folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9893131-0a95-4472-aa42-a73bd8d50576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_DATASET_FOLDER = '/data/projects/swat/datasets_src/ImageNet/ILSVRC2012_img_val_TFrecords'\n",
    "TRAIN_DATASET_FOLDER = '/data/projects/swat/datasets_src/ImageNet/ILSVRC2012_img_train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028112db-3143-4fcb-96ae-e639e6476c31",
   "metadata": {},
   "source": [
    "Let us define few helper functions to load the evaluation dataset, unlabeled represantative dataset (for quantization calibration) and to perform preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef7c875-c4fc-4819-97e5-721805cba546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_dataset():\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=TEST_DATASET_FOLDER,\n",
    "        batch_size=50,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=False,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0408f624-ab68-4989-95f8-f9d327882840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_dataset():\n",
    "    print('loading dataset, this may take few minutes ...')\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=TRAIN_DATASET_FOLDER,\n",
    "        batch_size=50,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "\n",
    "    def representative_dataset():\n",
    "        return dataset.take(1).get_single_element()[0].numpy()\n",
    "\n",
    "    return representative_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b",
   "metadata": {},
   "source": [
    "## Floating point model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b67fad-7edd-434d-af52-eb965d009fc3",
   "metadata": {},
   "source": [
    "Now we would like to evaluate the 32-bits FP precision model, show how to compress it into 8-bits precision model, and check the effect on the model accuracy.\n",
    "\n",
    "First, we need to load the evaluation dataset from ImageNet folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd661b39-e033-4efc-a916-f97a1642cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 1000 classes.\n"
     ]
    }
   ],
   "source": [
    "evaluation_dataset = get_validation_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401347d-02c6-4f52-87e7-252cd1b7c5d7",
   "metadata": {},
   "source": [
    "Secondly, load pre-trained mobilenet-v2 model, in a floating-point precision format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80cac59f-ec5e-41ca-b673-96220924a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889d217-90a6-4615-8569-38dc9cdd5999",
   "metadata": {},
   "source": [
    "then evaluate the model using the evaluation dataset. note that we need to compile the model before evaluation and set the loss and the evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 36s 34ms/step - loss: 1.2200 - accuracy: 0.7185\n",
      "Float model accuracy: 0.7185199856758118\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "results = model.evaluate(evaluation_dataset)\n",
    "print('Float model accuracy: ' + str(results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe71db-7508-4046-b426-bed9958fd9f7",
   "metadata": {},
   "source": [
    "## Model quantization using MCT and re-evaluation the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdd564-56ea-494e-80c2-e0a8b777cec3",
   "metadata": {},
   "source": [
    "Next, we would like to quantize the model using MCT. To do so, we need to define representative dataset generator, which is a function that returns a list of unlabeled images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de9a80df-30b4-4e51-ae46-7b00979908f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset, this may take few minutes ...\n",
      "Found 1281167 files belonging to 1000 classes.\n"
     ]
    }
   ],
   "source": [
    "representative_dataset_gen = get_representative_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9",
   "metadata": {},
   "source": [
    "then, to apply the hardware-friendly post training quantization on the model. we use 10 iterations for calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:01<00:00,  6.15s/it]\n"
     ]
    }
   ],
   "source": [
    "quantized_model, quantization_info = mct.keras_post_training_quantization(model, representative_dataset_gen, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c",
   "metadata": {},
   "source": [
    "Finally, we evaluate the quantized model performance and note to the small accuracy gap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 63s 62ms/step - loss: 1.3608 - accuracy: 0.7160\n",
      "Quantized model accuracy: 0.7160000205039978\n"
     ]
    }
   ],
   "source": [
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "results = quantized_model.evaluate(evaluation_dataset)\n",
    "print('Quantized model accuracy: ' + str(results[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
